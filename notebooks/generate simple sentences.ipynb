{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sentence Generation\n",
    "This notebook demonstrates how to use an RNN paired with rule-based corpus sentence extraction to train a generative model for creating simple sentences. This model can be used for eg generation of simple practice sentences for language learning applications. Here the generative model is mainly employed as a convenience for storing and reproducing the prototypes identified by the corpus-based approach, although one maintains the ability to apply minor perturbations to generated text without the need to hard-code exhaustive grammatical rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from spacy.matcher import Matcher\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "passive_matcher = Matcher(nlp.vocab)\n",
    "passive_rule = [{'DEP':'nsubjpass'}, {'DEP': 'auxpass'}]\n",
    "passive_matcher.add('Passive',[passive_rule])\n",
    "\n",
    "clause_matcher = Matcher(nlp.vocab)\n",
    "clause_rule = [{'DEP':'det', 'OP': '*'}, {'DEP':'amod', 'OP': '*'}, \n",
    "               {'DEP':'nsubj'}, {'DEP':'ROOT'}, {'DEP':'det', 'OP': '*'}, \n",
    "               {'DEP': 'dobj', 'OP': '*'}, {'DEP': 'prep'}, {'DEP': 'pobj'}]\n",
    "clause_matcher.add('Clause',[clause_rule])\n",
    "\n",
    "pp_matcher = Matcher(nlp.vocab)\n",
    "pp_rule = [{'DEP':'prep'}, {'DEP':'det', 'OP': '*'}, {'DEP':'amod', 'OP': '*'}, {'DEP':'pobj'}]\n",
    "pp_matcher.add('PrepPhrase',[pp_rule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('res/verb_table.csv')\n",
    "corpus_filename = 'D:\\\\archive\\\\20200625\\\\en-es.txt\\\\subtitles.en'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class definitions.\n",
    "class Tense(Enum):\n",
    "    SIMPLEPRES = 0\n",
    "    SIMPLEPAST = 1\n",
    "    PRESPERFECT = 2\n",
    "    PASTPERFECT = 3\n",
    "    PRESPROG = 4\n",
    "    PASTPROG = 5\n",
    "    PASTPERFPROG = 6\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self == Tense.SIMPLEPRES:\n",
    "            return 'Simple present'\n",
    "        if self == Tense.SIMPLEPAST:\n",
    "            return 'Simple past'\n",
    "        if self == Tense.PRESPERFECT:\n",
    "            return 'Present perfect'\n",
    "        if self == Tense.PASTPERFECT:\n",
    "            return 'Past perfect'\n",
    "        if self == Tense.PRESPROG:\n",
    "            return 'Present progressive'\n",
    "        if self == Tense.PASTPROG:\n",
    "            return 'Past progressive'\n",
    "        if self == Tense.PASTPERFPROG:\n",
    "            return 'Past perfect progressive'\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    \n",
    "\n",
    "class Person(Enum):\n",
    "    I = 0\n",
    "    WETHEY = 1\n",
    "    YOU = 2\n",
    "    HSI = 3\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self == Person.I:\n",
    "            return 'I'\n",
    "        if self == Person.WETHEY:\n",
    "            return 'We | they'\n",
    "        if self == Person.YOU:\n",
    "            return 'You'\n",
    "        if self == Person.HSI:\n",
    "            return 'He | she | it'\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    \n",
    "\n",
    "class Verb:\n",
    "    def __init__(self, df_row):\n",
    "        self.lemma = df_row['lemma']\n",
    "        # Organize tenses and person in dictionary with (tense, person) key\n",
    "        self.forms = {}\n",
    "        # Simple present for all four persons\n",
    "        self.forms[(Tense.SIMPLEPRES, Person.I)] = df_row['i_pres']\n",
    "        self.forms[(Tense.SIMPLEPRES, Person.WETHEY)] = df_row['wethey_pres']\n",
    "        self.forms[(Tense.SIMPLEPRES, Person.YOU)] = df_row['you_pres']\n",
    "        self.forms[(Tense.SIMPLEPRES, Person.HSI)] = df_row['hsi_pres']\n",
    "        # Simple past for all four persons\n",
    "        self.forms[(Tense.SIMPLEPAST, Person.I)] = df_row['i_past']\n",
    "        self.forms[(Tense.SIMPLEPAST, Person.WETHEY)] = df_row['wethey_past']\n",
    "        self.forms[(Tense.SIMPLEPAST, Person.YOU)] = df_row['you_past']\n",
    "        self.forms[(Tense.SIMPLEPAST, Person.HSI)] = df_row['hsi_past']\n",
    "        # Other tenses can be formed by the combination of present or past participle with corresponding form of \"be\"\n",
    "        # Persent perfect\n",
    "        self.forms[(Tense.PRESPERFECT, Person.I)] = 'have ' + df_row['pastpart']\n",
    "        self.forms[(Tense.PRESPERFECT, Person.WETHEY)] = 'have ' + df_row['pastpart']\n",
    "        self.forms[(Tense.PRESPERFECT, Person.YOU)] = 'have ' + df_row['pastpart']\n",
    "        self.forms[(Tense.PRESPERFECT, Person.HSI)] = 'has ' + df_row['pastpart']\n",
    "        # Past perfect\n",
    "        self.forms[(Tense.PASTPERFECT, Person.I)] = 'had ' + df_row['pastpart']\n",
    "        self.forms[(Tense.PASTPERFECT, Person.WETHEY)] = 'had ' + df_row['pastpart']\n",
    "        self.forms[(Tense.PASTPERFECT, Person.YOU)] = 'had ' + df_row['pastpart']\n",
    "        self.forms[(Tense.PASTPERFECT, Person.HSI)] = 'had ' + df_row['pastpart']\n",
    "        # Present progressive\n",
    "        self.forms[(Tense.PRESPROG, Person.I)] = 'am ' + df_row['prespart']\n",
    "        self.forms[(Tense.PRESPROG, Person.WETHEY)] = 'are ' + df_row['prespart']\n",
    "        self.forms[(Tense.PRESPROG, Person.YOU)] = 'are ' + df_row['prespart']\n",
    "        self.forms[(Tense.PRESPROG, Person.HSI)] = 'is ' + df_row['prespart']\n",
    "        # Past progressive\n",
    "        self.forms[(Tense.PRESPROG, Person.I)] = 'was ' + df_row['prespart']\n",
    "        self.forms[(Tense.PRESPROG, Person.WETHEY)] = 'were ' + df_row['prespart']\n",
    "        self.forms[(Tense.PRESPROG, Person.YOU)] = 'were ' + df_row['prespart']\n",
    "        self.forms[(Tense.PRESPROG, Person.HSI)] = 'was ' + df_row['prespart']\n",
    "        # Past perfect progressive\n",
    "        self.forms[(Tense.PASTPERFPROG, Person.I)] = 'have been ' + df_row['prespart']\n",
    "        self.forms[(Tense.PASTPERFPROG, Person.WETHEY)] = 'have been ' + df_row['prespart']\n",
    "        self.forms[(Tense.PASTPERFPROG, Person.YOU)] = 'have been ' + df_row['prespart']\n",
    "        self.forms[(Tense.PASTPERFPROG, Person.HSI)] = 'has been ' + df_row['prespart']\n",
    "\n",
    "    def get_form(tense, person):\n",
    "        ''' Return the correct verb phrase given the tense and the form '''\n",
    "        return self.forms[tense, person]\n",
    "   \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions.\n",
    "def get_examples(lemma, verb_set):\n",
    "    examples = []     \n",
    "    with open(corpus_filename,'rb') as f:\n",
    "        line = f.readline()\n",
    "        cnt = 1\n",
    "        while line:\n",
    "            line_formatted = line.strip().decode('utf-8').replace('\"', '').replace('-', '')\n",
    "\n",
    "            if any(i in line_formatted for i in verb_set):\n",
    "                examples.append(line_formatted.lower())\n",
    "            line = f.readline()\n",
    "            cnt += 1\n",
    "            if cnt % 1000000 == 0:\n",
    "                print('.', end='')\n",
    "                    \n",
    "                        \n",
    "    return examples\n",
    "\n",
    "\n",
    "def cosine_sim_vectors(vec1, vec2):\n",
    "    vec1 = vec1.reshape(1, -1)\n",
    "    vec2 = vec2.reshape(1, -1)\n",
    "    \n",
    "    return cosine_similarity(vec1, vec2)[0][0]\n",
    "\n",
    "\n",
    "def get_pieces(examples_list, lemma, verbose=False):\n",
    "    \n",
    "    pieces = []\n",
    "    \n",
    "    lemma_matcher = Matcher(nlp.vocab)\n",
    "    lemma_rule = [{'DEP':'ROOT', 'LEMMA': lemma, 'POS': 'VERB'}]\n",
    "    lemma_matcher.add('LemmaInclusive',None,lemma_rule)\n",
    "    \n",
    "    for item in examples_list:\n",
    "        doc = nlp(item)\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            sent_split = sent.text\n",
    "            sent_split_doc = nlp(sent_split)\n",
    "            clause = clause_matcher(sent_split_doc)\n",
    "            lemma_check = lemma_matcher(sent_split_doc)        \n",
    "        \n",
    "            if clause and lemma_check and '?' not in sent_split:\n",
    "                original = sent_split\n",
    "                d_np = []\n",
    "                m_np = []\n",
    "                h_np = []\n",
    "\n",
    "                verb = []\n",
    "\n",
    "                d_vp = []\n",
    "                m_vp = []\n",
    "                h_vp = []\n",
    "\n",
    "                prep = []\n",
    "\n",
    "                d_pp = []\n",
    "                m_pp = []\n",
    "                h_pp = []\n",
    "\n",
    "                for tok in sent_split_doc:\n",
    "                    if tok.dep_ == 'nsubj':     \n",
    "                        d_np = [desc for desc in tok.subtree if (desc.dep_ == \"det\")]\n",
    "                        m_np = [desc for desc in tok.subtree if (desc.dep_ == \"amod\")]    \n",
    "                        h_np = [tok]\n",
    "\n",
    "                    elif tok.dep_ == 'ROOT':\n",
    "                        verb = [tok]\n",
    "\n",
    "                    elif tok.dep_ == 'dobj':\n",
    "                        d_vp = [desc for desc in tok.subtree if (desc.dep_ == \"det\")]\n",
    "                        m_vp = [desc for desc in tok.subtree if (desc.dep_ == \"amod\")]\n",
    "                        h_vp = [tok]\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                p_phrase = pp_matcher(sent_split_doc)\n",
    "                if p_phrase:\n",
    "                    for match_id, start, end in p_phrase:\n",
    "                        string_id = nlp.vocab.strings[match_id]\n",
    "                        span = doc[start:end]\n",
    "                        p_doc = nlp(span.text)\n",
    "                    prep = [[tok for tok in p_doc if (tok.dep_ == \"ROOT\" or 'prep')][0]]  \n",
    "                    d_pp = [tok for tok in p_doc if (tok.dep_ == \"det\")]\n",
    "                    m_pp = [tok for tok in p_doc if (tok.dep_ == \"amod\")]\n",
    "                    h_pp = [tok for tok in p_doc if (tok.dep_ == \"pobj\")]\n",
    "\n",
    "\n",
    "                dict_pieces = {\n",
    "\n",
    "                        'd_np': d_np,\n",
    "                        'm_np': m_np,\n",
    "                        'h_np': h_np,\n",
    "                        'verb': verb,\n",
    "                        'd_vp': d_vp,\n",
    "                        'm_vp': m_vp,\n",
    "                        'h_vp': h_vp,\n",
    "                        'prep': prep,\n",
    "                        'd_pp': d_pp,\n",
    "                        'm_pp': m_pp,\n",
    "                        'h_pp': h_pp\n",
    "\n",
    "                }\n",
    "                \n",
    "                pieces_cleaned = []\n",
    "                # This will need to be altered so we can also account for empty objects etc.\n",
    "                for key, value in dict_pieces.items():\n",
    "                    if len(value) > 0:\n",
    "                        if key == 'verb':\n",
    "                            pieces_cleaned.append(value[0].text)\n",
    "                        else:\n",
    "                            pieces_cleaned.append(value[0].text)\n",
    "                        \n",
    "                proposed = ' '.join(pieces_cleaned) + \".\"\n",
    "\n",
    "                vectorizer = CountVectorizer().fit_transform([original, proposed])\n",
    "                vectors = vectorizer.toarray()\n",
    "                csim = cosine_similarity(vectors)\n",
    "                similarity = cosine_sim_vectors(vectors[0], vectors[1])\n",
    "                \n",
    "                if similarity > 0.84:\n",
    "                    d = {'source': original, 'parsed': proposed, 'similarity': similarity}\n",
    "                    pieces.append(d)\n",
    "                    if verbose is True:\n",
    "                        print('\\t' + proposed)\n",
    "                        print('\\n')\n",
    "                    \n",
    "    return pieces\n",
    "\n",
    "\n",
    "def export_examples(examples_list, lemma):\n",
    "    filedata = 'example_sentences_lemmata.csv'\n",
    "    fieldnames = ['lemma', 'examples']\n",
    "    examples = []\n",
    "    with open(filedata, 'a', newline='', encoding='utf-8') as f1:\n",
    "        csv_output1 = csv.DictWriter(f1, delimiter=',', fieldnames=fieldnames)\n",
    "        for item in examples_list:\n",
    "            examples.append(item['parsed'])\n",
    "        csv_output1.writerow({'lemma': lemma, 'examples': examples})\n",
    "        \n",
    "        \n",
    "def export_sentences_flat(examples):\n",
    "    with open('res/example_sentences_flat.txt', 'a') as f:\n",
    "        for item in examples:\n",
    "            f.write(item['parsed'])\n",
    "            f.write('\\n')\n",
    "            \n",
    "def build_corpus(verbs, start, end):\n",
    "    # reset the sentences corpus\n",
    "    with open('res/example_sentences_flat.txt', 'w') as f:\n",
    "        f.write('')\n",
    "        \n",
    "    for verb in tqdm(verbs[start:end]):\n",
    "        lemma = verb.lemma\n",
    "        verb_forms = verb.forms.items()\n",
    "        verb_set = set()\n",
    "        for key, value in verb_forms:\n",
    "            verb_set.add(value)\n",
    "\n",
    "        print('Examining verb: {}'.format(lemma))\n",
    "        example_sentences = get_examples(lemma, verb_set)\n",
    "        print('\\tNumber of examples: {}\\n'.format(len(example_sentences)))\n",
    "        print('\\nExtracting plausible examples ...\\n')\n",
    "        pieces = get_pieces(example_sentences, lemma)\n",
    "        pieces_sorted = sorted(pieces, key=lambda k: k['similarity'], reverse=True)\n",
    "        print('Number of plausible example sentences: {}'.format(len(pieces_sorted)))\n",
    "        export_sentences_flat(pieces_sorted)\n",
    "        \n",
    "        \n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "def text_from_ids(ids, id_to_char):\n",
    "    return tf.strings.reduce_join(id_to_char(ids), axis=-1)\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    with open('res/example_sentences_flat.txt', 'r', encoding='utf-8') as f:\n",
    "        texts = f.read().splitlines()\n",
    "        \n",
    "    text = ' '.join([t.ljust(100, '.') for t in texts])\n",
    "    vocab = sorted(set(text))\n",
    "    chars = tf.strings.unicode_split(texts, input_encoding='UTF-8')\n",
    "    char_to_id = preprocessing.StringLookup(\n",
    "        vocabulary=list(vocab),\n",
    "        mask_token=None,\n",
    "    )\n",
    "    id_to_char = preprocessing.StringLookup(\n",
    "        vocabulary=char_to_id.get_vocabulary(),\n",
    "        invert=True,\n",
    "        mask_token=None,\n",
    "    )\n",
    "    # pad all texts to 100 so we can easily batch the entire dataset using tf methods\n",
    "    all_ids = char_to_id(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "    seq_length = 100\n",
    "    examples_per_epoch = len(text)//(seq_length+1)\n",
    "    sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "    dataset = sequences.map(split_input_target)\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = 256\n",
    "    rnn_units = 1024\n",
    "    BUFFER_SIZE = 10000\n",
    "    dataset = (\n",
    "        dataset\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "    model = Model(\n",
    "        # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "        vocab_size=len(char_to_id.get_vocabulary()),\n",
    "        embedding_dim=embedding_dim,\n",
    "        rnn_units=rnn_units)\n",
    "        \n",
    "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer='adam', loss=loss)\n",
    "    history = model.fit(dataset, epochs=30, callbacks=[])\n",
    "    \n",
    "    return id_to_char, char_to_id, model\n",
    "\n",
    "\n",
    "def predict_with_onestep(query, temperature, verbose=False):\n",
    "    one_step_model = OneStep(model, id_to_char, char_to_id, temperature=temperature)\n",
    "    start = time.time()\n",
    "    states = None\n",
    "    next_char = tf.constant([query])\n",
    "    result = [next_char]\n",
    "\n",
    "    for n in range(100):\n",
    "        next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "        result.append(next_char)\n",
    "        if next_char == '.':\n",
    "            break\n",
    "\n",
    "    result = tf.strings.join(result)\n",
    "    end = time.time()\n",
    "    print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "    if verbose is True:\n",
    "        print('\\nRun time:', end - start)\n",
    "        \n",
    "    return one_step_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbs = []\n",
    "for index, row in df.iterrows():\n",
    "    verbs.append(Verb(row))\n",
    "    \n",
    "build_corpus(verbs, 4, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('res/example_sentences.csv')\n",
    "with open('res/example_sentences_flat.txt', 'w') as f:\n",
    "    for tpl in df.itertuples():\n",
    "        for text in eval(tpl.examples):\n",
    "            f.write(text.lower())\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "381/381 [==============================] - 64s 160ms/step - loss: 0.8779\n",
      "Epoch 2/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.3917\n",
      "Epoch 3/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.3187\n",
      "Epoch 4/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.2979\n",
      "Epoch 5/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.2701\n",
      "Epoch 6/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.2490\n",
      "Epoch 7/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.2371\n",
      "Epoch 8/30\n",
      "381/381 [==============================] - 62s 161ms/step - loss: 0.2208\n",
      "Epoch 9/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.2128\n",
      "Epoch 10/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1957\n",
      "Epoch 11/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1838\n",
      "Epoch 12/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1731\n",
      "Epoch 13/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1649\n",
      "Epoch 14/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1562\n",
      "Epoch 15/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1458\n",
      "Epoch 16/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1393\n",
      "Epoch 17/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1325\n",
      "Epoch 18/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1305\n",
      "Epoch 19/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1236\n",
      "Epoch 20/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1202\n",
      "Epoch 21/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1190\n",
      "Epoch 22/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1149\n",
      "Epoch 23/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1132\n",
      "Epoch 24/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1114\n",
      "Epoch 25/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1100\n",
      "Epoch 26/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1084\n",
      "Epoch 27/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1090\n",
      "Epoch 28/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1065\n",
      "Epoch 29/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1070\n",
      "Epoch 30/30\n",
      "381/381 [==============================] - 62s 160ms/step - loss: 0.1065\n"
     ]
    }
   ],
   "source": [
    "id_to_char, char_to_id, model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "he shaved the faces of gentlemen. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "0.3111111111111111\n",
      "he shaved the faces of gentlemen. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "0.5222222222222223\n",
      "he gave a measure of sugar. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "0.7333333333333333\n",
      "he drove boss for ferraldo. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "0.9444444444444444\n",
      "he fell in love. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "1.1555555555555557\n",
      "he robbed the science of someone. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "1.3666666666666667\n",
      "he owns the girl for number. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "1.577777777777778\n",
      "he tires the city of government. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "1.788888888888889\n",
      "he brought it by higlers. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "2.0\n",
      "he found behind it. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# increasing temperature encourages the model to deviate from the training examples\n",
    "for v in np.linspace(0.1, 2, 10):\n",
    "    print(v)\n",
    "    predict_with_onestep(\"he \", v)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he bled to death. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "he practiced karate on the weekends. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "he shares with me. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "he found a picture of you. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "he fell in love. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "he fell in love. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "he slipped by me. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "he retired in florida. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "he brought this on himself. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n",
      "he forgot about it. \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can now easily generate a variety of grammatical sentences\n",
    "for i in range(10):\n",
    "    predict_with_onestep(\"he \", 0.5)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x0000026386663908>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as generate_one_step while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as generate_one_step while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: res/one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: res/one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(OneStep(model, id_to_char, char_to_id), 'res/one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
