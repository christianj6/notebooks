{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram Vector Text Classification\n",
    "This notebook demonstrates an alternative approach to feature extraction which is simply a character ngram tfidf-vector. The goal of this demonstration is to show how effective this relatively simple feature extraction approach can be, as an alternative to more intensive methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian\\Anaconda3\\envs\\deepsight\\lib\\site-packages\\dask\\dataframe\\utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sparse_dot_topn import awesome_cossim_topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv('res/bbc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(string, n=2):\n",
    "    '''\n",
    "    Custom analyzer for tfidf vectorizer. Splits a\n",
    "    string into all possible ngrams.\n",
    "    Parameters\n",
    "    ---------\n",
    "        string : str\n",
    "            String for which we wish to obtain the\n",
    "            ngrams.\n",
    "        n : int\n",
    "            Number of grams.\n",
    "    Returns\n",
    "    ---------\n",
    "        ngrams : list[str]\n",
    "            List of ngrams for the string.\n",
    "    '''\n",
    "    # Clean the string.\n",
    "    string = re.sub(r'[,-/]|\\sBD',r'', string)\n",
    "    # Get the character ngrams.\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "class Keywords():\n",
    "    '''\n",
    "    Object for storing and matching\n",
    "    to a predefined keyword list.\n",
    "    Attributes\n",
    "    ---------\n",
    "        words : list\n",
    "            Words against which we\n",
    "            want to match.\n",
    "        ids : list (optional)\n",
    "            List of meaningful\n",
    "            ids for alternative\n",
    "            retrieval of matched words.\n",
    "    Methods\n",
    "    ---------\n",
    "        match\n",
    "            Match to a single\n",
    "            word.\n",
    "    '''\n",
    "    def __init__(self, words:list, ids:list=None):\n",
    "        '''\n",
    "        Initialize ie train the keyword\n",
    "        matcher.\n",
    "        '''\n",
    "        # Tfidf vectorizer.\n",
    "        self.vectorizer = None\n",
    "        # Tfidf matrix.\n",
    "        self.matrix = None\n",
    "        # Wordlist.\n",
    "        self.words = words\n",
    "        # Ids if provided.\n",
    "        if ids:\n",
    "            self.ids = ids\n",
    "        else:\n",
    "            self.ids = range(len(words))\n",
    "\n",
    "        # Fit object to words.\n",
    "        self.fit(words)\n",
    "\n",
    "\n",
    "    def fit(self, words:list):\n",
    "        '''\n",
    "        Fit a tfidf matrix to the wordlist,\n",
    "        which can then be used to match\n",
    "        strings back to words from the list.\n",
    "        Parameters\n",
    "        ---------\n",
    "            words : list\n",
    "                List of words for\n",
    "                computing the matrix.\n",
    "        '''\n",
    "        # Do some mild preprocessing.\n",
    "        strings = list(map(lambda x: str(x).lower(), words))\n",
    "        # Create the tfidf vectorizer with\n",
    "        # ngram analyzer.\n",
    "        self.vectorizer = TfidfVectorizer(analyzer=ngrams)\n",
    "        # Fit the vectorizer to the data.\n",
    "        self.matrix = self.vectorizer.fit_transform(strings)\n",
    "\n",
    "\n",
    "    def get_vector(self, string:str):\n",
    "        '''\n",
    "        Get ngram vector representation of a\n",
    "        string based on the fitted vectorizer.\n",
    "        Parameters\n",
    "        ---------\n",
    "            string : str\n",
    "                String to transform.\n",
    "        Returns\n",
    "        ---------\n",
    "            vector : csr_matrix\n",
    "                Compressed sparse row matrix\n",
    "                representation.\n",
    "        '''\n",
    "        # Cast the query string into a list because sklearn vectorizer\n",
    "        # wants an array-like object.\n",
    "        string = [string]\n",
    "        # Vectorize the string so we can match it against the matrix.\n",
    "        vector = self.vectorizer.transform(string)\n",
    "\n",
    "        return vector\n",
    "    \n",
    "    def match(self, string:str, bound:float=0.7):\n",
    "        '''\n",
    "        Match a query string against the list\n",
    "        of fitted strings via the tfidf matrix.\n",
    "        Parameters\n",
    "        ---------\n",
    "            string : list\n",
    "                List with single string to which we wish to match strings from the\n",
    "                source list.\n",
    "            bound : float\n",
    "                Lower bound below which we will ignore\n",
    "                matched strings.\n",
    "        Returns\n",
    "        ---------\n",
    "            matches : list[tuple]\n",
    "                List of tuples containing the original string, the matched\n",
    "                string, and the cosine distance between their tfidf vectors.\n",
    "        '''\n",
    "        def cossim_top(query_matrix, ntop=10, lower_bound=0):\n",
    "            '''\n",
    "            Returns csr matrix with topn matches to the fitted\n",
    "            matrix for for a given query matrix\n",
    "            Parameters\n",
    "            ---------\n",
    "                query_matrix : np.array\n",
    "                    Vectorized matrix for which we wish to\n",
    "                    identify close neighbors.\n",
    "                ntop : int\n",
    "                    Number of top matches to grab to speed up\n",
    "                    processing.\n",
    "                lower_bound : float\n",
    "                    Lower distance bound below which we will\n",
    "                    ignore matches.\n",
    "            Returns\n",
    "            ---------\n",
    "                matches : csr_matrix\n",
    "            '''\n",
    "            # Force the query and target into csr matrices. If they\n",
    "            # are already, there is no overhead.\n",
    "            A = self.matrix.tocsr()\n",
    "            B = query_matrix.tocsr()\n",
    "\n",
    "            return awesome_cossim_topn(A, B.transpose(), ntop, lower_bound)\n",
    "\n",
    "        # Vectorize the string so we can match it against the matrix.\n",
    "        query = self.get_vector(string)\n",
    "        # Search for top matches. Ignore those which are too small.\n",
    "        matches = cossim_top(query, lower_bound=bound)\n",
    "        # Get all the non-zero matches.\n",
    "        non_zero = matches.nonzero()\n",
    "        # Get the ids of the non-zero matches to map back to words.\n",
    "        match_ids = non_zero[0]\n",
    "        # Return as a list of tuples.\n",
    "        matches = [(self.words[j], matches.data[i], j) for i, j in enumerate(match_ids)]\n",
    "        # Sort it by closest match.\n",
    "        return sorted(matches, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.text.values\n",
    "kw = Keywords(train_x.tolist())\n",
    "train_x_sparse = [kw.get_vector(x).A for x in train_x.tolist()]\n",
    "train_x_np = np.array(train_x_sparse).reshape(len(train_x), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# linear svc\n",
    "results = []\n",
    "kfold = KFold(shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(train_x, train_y):\n",
    "    X = train_x_np[train_index]\n",
    "    y = train_y[train_index]\n",
    "    clf = LinearSVC()\n",
    "    clf.fit(X, y)\n",
    "    preds = clf.predict(train_x_np[test_index])\n",
    "    results.append(accuracy_score(train_y[test_index], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.955, 0.9475, 0.94, 0.965, 0.9375]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching\n",
    "results = []\n",
    "kfold = KFold(shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(train_x, train_y):\n",
    "    X = train_x[train_index]\n",
    "    y = train_y[train_index]\n",
    "    clf = Keywords(X.tolist())\n",
    "    preds = [y[clf.match(string)[0][2]] if len(clf.match(string)) > 0 else -5 for string in train_x[test_index].tolist()]\n",
    "    results.append(accuracy_score(train_y[test_index], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.82, 0.865, 0.8375, 0.8825, 0.825]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
