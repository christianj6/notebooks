{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Fine Tune Transformers\n",
    "This notebook demonstrates how to finetune a transformers model for text classification by appending layers to a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel, BatchEncoding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL = 'distilbert-base-uncased'\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, encodings: BatchEncoding, labels: list):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(val[idx]) for key, val in self.encodings.items()\n",
    "        }\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    \n",
    "    MODEL_FILENAME = 'model.pb'\n",
    "    \n",
    "    def __init__(self, transformer, tokenizer, n, max_seq_len = MAX_LEN):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.pooling_layer = torch.nn.Linear(\n",
    "            self.transformer.config.dim, self.transformer.config.dim\n",
    "        )\n",
    "        self.dropout_layer = torch.nn.Dropout(0.3)\n",
    "        self.classifier_layer = torch.nn.Linear(\n",
    "            self.transformer.config.dim, n\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        self.to(DEVICE)\n",
    "        self.n = n\n",
    "        \n",
    "    def save(self, path: str):\n",
    "        with open(path, 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_state = self.transformer(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )[0]\n",
    "        pooled_output = hidden_state[:, 0]\n",
    "        pooled_output = self.pooling_layer(pooled_output)\n",
    "        pooled_output = torch.nn.ReLU()(pooled_output)\n",
    "        pooled_output = self.dropout_layer(pooled_output)\n",
    "        pooled_output = self.classifier_layer(pooled_output)\n",
    "        \n",
    "        return pooled_output\n",
    "    \n",
    "    def learn(\n",
    "        self,\n",
    "        data: torch.utils.data.Dataset,\n",
    "        verbose: bool = False,\n",
    "        epochs: int = 3,\n",
    "        batch_size: int = 4,\n",
    "        lr: float = 5e-5,\n",
    "    ):\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            data, batch_size=batch_size, shuffle=True\n",
    "        )\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n",
    "        self.train()\n",
    "        for i in range(epochs):\n",
    "            for j, batch in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "                outputs = self(input_ids, attention_mask=attention_mask)\n",
    "                loss = torch.nn.BCEWithLogitsLoss()(outputs, labels)\n",
    "                if verbose and j % 5000 == 0:\n",
    "                    print(f\"Epoch {i+1}, Loss: {loss.item()}\")\n",
    "                    \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                        \n",
    "    def predict(\n",
    "        self,\n",
    "        sequence: list,\n",
    "    ) -> list:\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        for text in sequence:\n",
    "            inputs = self.tokenizer(\n",
    "                text, truncation=True, padding='max_length', return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "            pred = (\n",
    "                self(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "                .to(\"cpu\")\n",
    "                .clone()\n",
    "                .detach()\n",
    "                .numpy()\n",
    "                .tolist()\n",
    "            )\n",
    "            y = [0] * self.n\n",
    "            y[pred[0].index(max(pred[0]))] = 1\n",
    "            predictions.append(y)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_y(data, dataset):\n",
    "    return data.label.apply(lambda x: [1 if y in x else 0 for y in dataset.label.unique()]).tolist()\n",
    "\n",
    "def build_X(data, clf):\n",
    "    return clf.tokenizer(\n",
    "    data.text.tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('res/bbc.csv').groupby('label').sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "clf = Classifier(\n",
    "    transformer=DistilBertModel.from_pretrained(MODEL),\n",
    "    tokenizer=DistilBertTokenizerFast.from_pretrained(\n",
    "        MODEL, model_max_length=MAX_LEN,\n",
    "    ),\n",
    "    n=len(df.label.unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = build_y(train_df, df)\n",
    "y_test = build_y(test_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = build_X(train_df, clf)\n",
    "X_test = test_df.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TextDataset(\n",
    "    encodings=X_train,\n",
    "    labels=y_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian\\Anaconda3\\envs\\deepsight\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "clf.learn(data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
